{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Load pyspark\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# charting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"dengue\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train = (1456, 25)\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"data/\"\n",
    "\n",
    "df_features = spark.read.csv(path_to_data + \"dengue_features_train.csv\",\n",
    "                             header=True)\n",
    "df_labels = spark.read.csv(path_to_data + \"dengue_labels_train.csv\",\n",
    "                           header=True)\n",
    "\n",
    "#join df_features + df_labels\n",
    "df_train = df_features.join(df_labels, ['city', 'year', 'weekofyear'])\n",
    "\n",
    "print(\"df_train = ({}, {})\".format(df_train.count(), len(df_train.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      " |-- week_start_date: date (nullable = true)\n",
      " |-- ndvi_ne: float (nullable = true)\n",
      " |-- ndvi_nw: float (nullable = true)\n",
      " |-- ndvi_se: float (nullable = true)\n",
      " |-- ndvi_sw: float (nullable = true)\n",
      " |-- reanalysis_air_temp_k: float (nullable = true)\n",
      " |-- reanalysis_avg_temp_k: float (nullable = true)\n",
      " |-- reanalysis_dew_point_temp_k: float (nullable = true)\n",
      " |-- reanalysis_max_air_temp_k: float (nullable = true)\n",
      " |-- reanalysis_min_air_temp_k: float (nullable = true)\n",
      " |-- reanalysis_precip_amt_kg_per_m2: float (nullable = true)\n",
      " |-- reanalysis_relative_humidity_percent: float (nullable = true)\n",
      " |-- reanalysis_sat_precip_amt_mm: float (nullable = true)\n",
      " |-- reanalysis_specific_humidity_g_per_kg: float (nullable = true)\n",
      " |-- reanalysis_tdtr_k: float (nullable = true)\n",
      " |-- station_avg_temp_c: float (nullable = true)\n",
      " |-- station_diur_temp_rng_c: float (nullable = true)\n",
      " |-- station_max_temp_c: float (nullable = true)\n",
      " |-- station_min_temp_c: float (nullable = true)\n",
      " |-- station_precip_mm: float (nullable = true)\n",
      " |-- total_cases: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The 2 columns 'precipitation_amt_mm' and 'reanalysis_sat_precip_amt_mm' are the same\n",
    "# we drop 'precipitation_amt_mm'\n",
    "df_train = df_train.drop('precipitation_amt_mm')\n",
    "\n",
    "# recast 'week_start_date' as a date. Nice to have for plotting or time series analysis\n",
    "df_train = df_train.withColumn('week_start_date', F.to_date('week_start_date', 'yyyy-MM-dd'))\n",
    "\n",
    "# recast 'year' and 'weekofyear' to integer\n",
    "df_train = df_train \\\n",
    "    .withColumn('year', df_train['year'].cast('int')) \\\n",
    "    .withColumn('weekofyear', df_train['weekofyear'].cast('int'))\n",
    "\n",
    "# cast columns to float\n",
    "for col_name in df_train.columns[4:-1]:\n",
    "    df_train = df_train.withColumn(col_name, df_train[col_name].cast('float'))\n",
    "\n",
    "# cast column total_cases as integer\n",
    "df_train = df_train.withColumn('total_cases', df_train['total_cases'].cast('int'))\n",
    "\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train = (1456, 24)\n",
      "city => 0\n",
      "year => 0\n",
      "weekofyear => 0\n",
      "week_start_date => 0\n",
      "ndvi_ne => 194\n",
      "ndvi_nw => 52\n",
      "ndvi_se => 22\n",
      "ndvi_sw => 22\n",
      "reanalysis_air_temp_k => 10\n",
      "reanalysis_avg_temp_k => 10\n",
      "reanalysis_dew_point_temp_k => 10\n",
      "reanalysis_max_air_temp_k => 10\n",
      "reanalysis_min_air_temp_k => 10\n"
     ]
    }
   ],
   "source": [
    "print(\"df_train = ({}, {})\".format(df_train.count(), len(df_train.columns)))\n",
    "\n",
    "# identify null value\n",
    "# ===================\n",
    "for col_name in df_train.columns:\n",
    "    print(\"{} => {}\".format(col_name,\n",
    "                            df_train.filter(F.isnull(df_train[col_name])).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null value with previous value\n",
    "w = Window().partitionBy().orderBy(F.col('week_start_date'))\n",
    "\n",
    "for col_name in df_train.columns[4:-1]:\n",
    "    df_train = df_train.withColumn(col_name, F.last(col_name, True).over(w))\n",
    "    \n",
    "# identify null value\n",
    "for col_name in df_train.columns:\n",
    "    print(\"{} => {}\".format(col_name,\n",
    "                            df_train.filter(F.isnull(df_train[col_name])).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "df_train_sj = df_train.filter(df_train.city == 'sj')\n",
    "min_date_sj, max_date_sj = df_train_sj.select(min(\"week_start_date\"), max(\"week_start_date\")).first()\n",
    "\n",
    "print(\"San Juan data:\")\n",
    "print(\"df_train_sj = ({}, {})\".format(df_train_sj.count(), len(df_train_sj.columns)))\n",
    "print(\"\\tdate_start = {} / date_stop = {}\".format(min_date_sj, max_date_sj))\n",
    "\n",
    "df_train_iq = df_train.filter(df_train.city == 'iq')\n",
    "min_date_iq, max_date_iq = df_train_iq.select(min(\"week_start_date\"), max(\"week_start_date\")).first()\n",
    "\n",
    "print(\"Iquitos data:\")\n",
    "print(\"df_train_iq = ({}, {})\".format(df_train_iq.count(), len(df_train_iq.columns)))\n",
    "print(\"\\tdate_start = {} / date_stop = {}\".format(min_date_iq, max_date_iq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_sj = df_train_sj.toPandas()\n",
    "pd_train_iq = df_train_iq.toPandas()\n",
    "\n",
    "# Total cases per city\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.grid()\n",
    "plt.title('Dengue Fever total cases per week')\n",
    "\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('total cases')\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "\n",
    "plt.plot(pd_train_sj['week_start_date'], pd_train_sj['total_cases'], label='San Juan')\n",
    "plt.plot(pd_train_iq['week_start_date'], pd_train_iq['total_cases'], label='Iquitos')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "fig.savefig(\"figs/01-fig_01.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert temperature from kelvin to degree\n",
    "\n",
    "df_train = df_train \\\n",
    "    .withColumn('reanalysis_air_temp_c', df_train['reanalysis_air_temp_k'] - 273.15) \\\n",
    "    .withColumn('reanalysis_avg_temp_c', df_train['reanalysis_avg_temp_k'] - 273.15) \\\n",
    "    .withColumn('reanalysis_dew_point_temp_c', df_train['reanalysis_dew_point_temp_k'] - 273.15) \\\n",
    "    .withColumn('reanalysis_max_air_temp_c', df_train['reanalysis_max_air_temp_k'] - 273.15) \\\n",
    "    .withColumn('reanalysis_min_air_temp_c', df_train['reanalysis_min_air_temp_k'] - 273.15) \\\n",
    "    .withColumn('reanalysis_tdtr_c', df_train['reanalysis_tdtr_k'] - 273.15)\n",
    "\n",
    "df_train = df_train \\\n",
    "    .drop('reanalysis_air_temp_k') \\\n",
    "    .drop('reanalysis_avg_temp_k') \\\n",
    "    .drop('reanalysis_dew_point_temp_k') \\\n",
    "    .drop('reanalysis_max_air_temp_k') \\\n",
    "    .drop('reanalysis_min_air_temp_k') \\\n",
    "    .drop('reanalysis_tdtr_k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a window\n",
    "w = Window().partitionBy().orderBy(F.col('week_start_date'))\n",
    "\n",
    "print(\"df_train = ({}, {})\".format(df_train.count(), len(df_train.columns)))\n",
    "\n",
    "# add new columns with different lags\n",
    "df_train = df_train \\\n",
    "    .withColumn('station_avg_temp_1_c', F.lag('station_avg_temp_c', count=1).over(w)) \\\n",
    "    .withColumn('station_avg_temp_2_c', F.lag('station_avg_temp_c', count=2).over(w)) \\\n",
    "    .withColumn('station_avg_temp_3_c', F.lag('station_avg_temp_c', count=3).over(w)) \\\n",
    "    .withColumn('station_avg_temp_4_c', F.lag('station_avg_temp_c', count=4).over(w)) \\\n",
    "    .withColumn('station_precip_1_mm', F.lag('station_precip_mm', count=1).over(w)) \\\n",
    "    .withColumn('station_precip_2_mm', F.lag('station_precip_mm', count=2).over(w)) \\\n",
    "    .withColumn('station_precip_3_mm', F.lag('station_precip_mm', count=3).over(w)) \\\n",
    "    .withColumn('station_precip_4_mm', F.lag('station_precip_mm', count=4).over(w)) \\\n",
    "    .withColumn('reanalysis_sat_precip_amt_1_mm', F.lag('reanalysis_sat_precip_amt_mm', count=1).over(w)) \\\n",
    "    .withColumn('reanalysis_sat_precip_amt_2_mm', F.lag('reanalysis_sat_precip_amt_mm', count=2).over(w)) \\\n",
    "    .withColumn('reanalysis_sat_precip_amt_3_mm', F.lag('reanalysis_sat_precip_amt_mm', count=3).over(w)) \\\n",
    "    .withColumn('reanalysis_sat_precip_amt_4_mm', F.lag('reanalysis_sat_precip_amt_mm', count=4).over(w))\n",
    "\n",
    "df_train = df_train \\\n",
    "    .withColumn('d_station_avg_temp_1_c', df_train['station_avg_temp_1_c'] - df_train['station_avg_temp_c']) \\\n",
    "    .withColumn('d_station_avg_temp_2_c', df_train['station_avg_temp_2_c'] - df_train['station_avg_temp_c']) \\\n",
    "    .withColumn('d_station_avg_temp_3_c', df_train['station_avg_temp_3_c'] - df_train['station_avg_temp_c']) \\\n",
    "    .withColumn('d_station_avg_temp_4_c', df_train['station_avg_temp_4_c'] - df_train['station_avg_temp_c']) \\\n",
    "    .withColumn('d_station_precip_1_mm', df_train['station_precip_1_mm'] - df_train['station_precip_mm']) \\\n",
    "    .withColumn('d_station_precip_2_mm', df_train['station_precip_2_mm'] - df_train['station_precip_mm']) \\\n",
    "    .withColumn('d_station_precip_3_mm', df_train['station_precip_3_mm'] - df_train['station_precip_mm']) \\\n",
    "    .withColumn('d_station_precip_4_mm', df_train['station_precip_4_mm'] - df_train['station_precip_mm']) \\\n",
    "    .withColumn('d_reanalysis_sat_precip_amt_1_mm', df_train['reanalysis_sat_precip_amt_1_mm'] - df_train['reanalysis_sat_precip_amt_mm']) \\\n",
    "    .withColumn('d_reanalysis_sat_precip_amt_2_mm', df_train['reanalysis_sat_precip_amt_2_mm'] - df_train['reanalysis_sat_precip_amt_mm']) \\\n",
    "    .withColumn('d_reanalysis_sat_precip_amt_3_mm', df_train['reanalysis_sat_precip_amt_3_mm'] - df_train['reanalysis_sat_precip_amt_mm']) \\\n",
    "    .withColumn('d_reanalysis_sat_precip_amt_4_mm', df_train['reanalysis_sat_precip_amt_4_mm'] - df_train['reanalysis_sat_precip_amt_mm'])\n",
    " \n",
    "print(\"df_train = ({}, {})\".format(df_train.count(), len(df_train.columns)))\n",
    "\n",
    "# we drop the four first rows\n",
    "df_train = df_train.dropna()\n",
    "print(\"df_train = ({}, {})\".format(df_train.count(), len(df_train.columns)))\n",
    "\n",
    "#df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed dataframe\n",
    "path_to_dir = \"data/preprocessed\"\n",
    "\n",
    "if os.path.exists(path_to_dir) and os.path.isdir(path_to_dir):\n",
    "    shutil.rmtree(path_to_dir)\n",
    "    \n",
    "df_train.write.parquet(path_to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
