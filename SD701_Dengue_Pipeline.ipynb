{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description des données\n",
    "\n",
    "Your goal is to predict the total_cases label for each (`city`, `year`, `weekofyear`) in the test set. \n",
    "There are two cities, *San Juan* and *Iquitos*, with test data for each city spanning 5 and 3 years respectively.\n",
    "You will make one submission that contains predictions for both cities.\n",
    "The data for each city have been concatenated along with a city column indicating the source: `sj` for San Juan and `iq` for Iquitos. \n",
    "The test set is a pure future hold-out, meaning the test data are sequential and non-overlapping with any of the training data.\n",
    "Throughout, missing values have been filled as `NaN`s.\n",
    "\n",
    "#### The features in this dataset\n",
    "\n",
    "You are provided the following set of information on a (`year`, `weekofyear`) timescale:\n",
    "\n",
    "(Where appropriate, units are provided as a `_unit` suffix on the feature name.)\n",
    "\n",
    "*City and date indicators*\n",
    "\n",
    " - `city` – City abbreviations: `sj` for San Juan and `iq` for Iquitos\n",
    " - `week_start_date` – Date given in yyyy-mm-dd format\n",
    "\n",
    "*NOAA's GHCN daily climate data weather station measurements*\n",
    "\n",
    " - `station_max_temp_c` – Maximum temperature\n",
    " - `station_min_temp_c` – Minimum temperature\n",
    " - `station_avg_temp_c` – Average temperature\n",
    " - `station_precip_mm` – Total precipitation\n",
    " - `station_diur_temp_rng_c` – Diurnal temperature range\n",
    " \n",
    "*PERSIANN satellite precipitation measurements (0.25x0.25 degree scale)*\n",
    "\n",
    " - `precipitation_amt_mm` – Total precipitation\n",
    "\n",
    "*NOAA's NCEP Climate Forecast System Reanalysis measurements (0.5x0.5 degree scale)*\n",
    "\n",
    " - `reanalysis_sat_precip_amt_mm` – Total precipitation\n",
    " - `reanalysis_dew_point_temp_k` – Mean dew point temperature\n",
    " - `reanalysis_air_temp_k` – Mean air temperature\n",
    " - `reanalysis_relative_humidity_percent` – Mean relative humidity\n",
    " - `reanalysis_specific_humidity_g_per_kg` – Mean specific humidity\n",
    " - `reanalysis_precip_amt_kg_per_m2` – Total precipitation\n",
    " - `reanalysis_max_air_temp_k` – Maximum air temperature\n",
    " - `reanalysis_min_air_temp_k` – Minimum air temperature\n",
    " - `reanalysis_avg_temp_k` – Average air temperature\n",
    " - `reanalysis_tdtr_k` – Diurnal temperature range\n",
    "\n",
    "*Satellite vegetation - Normalized difference vegetation index (NDVI) - NOAA's CDR Normalized Difference Vegetation Index (0.5x0.5 degree scale) measurements*\n",
    "\n",
    " - `ndvi_se` – Pixel southeast of city centroid\n",
    " - `ndvi_sw` – Pixel southwest of city centroid\n",
    " - `ndvi_ne` – Pixel northeast of city centroid\n",
    " - `ndvi_nw` – Pixel northwest of city centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pyspark\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# charting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"dengue\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"data/\"\n",
    "\n",
    "df_features = spark.read.csv(path_to_data + \"dengue_features_train.csv\",\n",
    "                             header=True)\n",
    "df_labels = spark.read.csv(path_to_data + \"dengue_labels_train.csv\",\n",
    "                           header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jointure\n",
    "\n",
    "Nous joignons les deux DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join\n",
    "df_train = df_features.join(df_labels, ['city', 'year', 'weekofyear'])\n",
    "\n",
    "print(\"df_train = ({}, {})\".format(df_train.count(), len(df_train.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 2 columns 'precipitation_amt_mm' and 'reanalysis_sat_precip_amt_mm' are the same\n",
    "# we drop 'precipitation_amt_mm'\n",
    "df_train = df_train.drop('precipitation_amt_mm')\n",
    "\n",
    "# recast 'week_start_date' as a date. Nice to have for plotting or time series analysis\n",
    "df_train = df_train.withColumn('week_start_date', F.to_date('week_start_date', 'yyyy-MM-dd'))\n",
    "\n",
    "# recast 'year' and 'weekofyear' to integer\n",
    "df_train = df_train \\\n",
    "    .withColumn('year', df_train['year'].cast('int')) \\\n",
    "    .withColumn('weekofyear', df_train['weekofyear'].cast('int'))\n",
    "\n",
    "# cast column to float\n",
    "for col_name in df_train.columns[4:]:\n",
    "    df_train = df_train.withColumn(col_name, df_train[col_name].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_train = ({}, {})\".format(df_train.count(), len(df_train.columns)))\n",
    "\n",
    "# identify null value\n",
    "# ===================\n",
    "for col_name in df_train.columns:\n",
    "    print(\"{} => {}\".format(col_name,\n",
    "                            df_train.filter(F.isnull(df_train[col_name])).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null value with previous value\n",
    "w = Window().partitionBy().orderBy(F.col('week_start_date'))\n",
    "\n",
    "for col_name in df_train.columns:\n",
    "    df_train = df_train.withColumn(col_name, F.last(col_name, True).over(w))\n",
    "    \n",
    "# identify null value\n",
    "for col_name in df_train.columns:\n",
    "    print(\"{} => {}\".format(col_name,\n",
    "                            df_train.filter(F.isnull(df_train[col_name])).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_train = ({}, {})\".format(df_train.count(), len(df_train.columns)))\n",
    "\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe from spark dataframe\n",
    "\n",
    "pd_train = df_train.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_sj = pd_train[pd_train['city'] == 'sj']\n",
    "pd_train_iq = pd_train[pd_train['city'] == 'iq']\n",
    "\n",
    "# Total cases per city\n",
    "plt.figure()\n",
    "\n",
    "plt.grid()\n",
    "plt.title('Dengue Fever total cases per week')\n",
    "\n",
    "\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('total cases')\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "\n",
    "plt.plot(pd_train_sj['week_start_date'], pd_train_sj['total_cases'], label='San Juan')\n",
    "plt.plot(pd_train_iq['week_start_date'], pd_train_iq['total_cases'], label='Iquitos')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buid a new column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction des jeux d'entrainement et de test (random split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random split\n",
    "train, test = df_train.randomSplit([0.75, 0.25], seed=18)\n",
    "\n",
    "train.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction du *pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='city', outputCol='city_')\n",
    "\n",
    "encoder = OneHotEncoder(inputCol='city_', outputCol='cityVect')\n",
    "\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=['year', 'weekofyear',\n",
    "               'ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw',\n",
    "               'reanalysis_air_temp_k','reanalysis_avg_temp_k',\n",
    "               'reanalysis_dew_point_temp_k', 'reanalysis_max_air_temp_k',\n",
    "               'reanalysis_min_air_temp_k', 'reanalysis_precip_amt_kg_per_m2',\n",
    "               'reanalysis_relative_humidity_percent', 'reanalysis_sat_precip_amt_mm',\n",
    "               'reanalysis_specific_humidity_g_per_kg', 'reanalysis_tdtr_k',\n",
    "               'station_avg_temp_c',\n",
    "               'station_diur_temp_rng_c',\n",
    "               'station_max_temp_c', 'station_min_temp_c', \n",
    "               'station_precip_mm',\n",
    "               'cityVect'], \n",
    "    outputCol = 'features')\n",
    "\n",
    "scaler = StandardScaler(inputCol='features',\n",
    "                        outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "lr = LinearRegression(featuresCol='scaled_features',\n",
    "                      labelCol='total_cases',\n",
    "                      predictionCol='lr_prediction')\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol='scaled_features',\n",
    "                           labelCol='total_cases',\n",
    "                           predictionCol='rf_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=[indexer, encoder, vectorAssembler, scaler, rf])\n",
    "\n",
    "evaluator_rf = RegressionEvaluator(labelCol='total_cases',\n",
    "                                   predictionCol='rf_prediction',\n",
    "                                   metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "\n",
    "model_rf = pipeline_rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prediction from 'test' dataset\n",
    "pred_rf = model_rf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf.select(['total_cases','rf_prediction']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator_rf.evaluate(pred_rf)\n",
    "\n",
    "print(\"Random Forest::Before grid search - RMSE = {:.3f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for random forest\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [2, 5, 10, 15, 20]) \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30, 40, 50]) \\\n",
    "    .build()\n",
    "\n",
    "crossval_rf = CrossValidator(estimator=pipeline_rf,\n",
    "                             estimatorParamMaps=paramGrid_rf,\n",
    "                             evaluator=evaluator_rf,\n",
    "                             numFolds=2,\n",
    "                             seed=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model_rf = crossval_rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prediction from 'test' dataset\n",
    "cv_pred_rf = cv_model_rf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator_rf.evaluate(cv_pred_rf)\n",
    "\n",
    "print(\"Random Forest::After grid search - RMSE = {:.3f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model_rf.bestModel.stages[-1].getNumTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe from spark dataframe\n",
    "pd_pred_rf = cv_pred_rf.toPandas()\n",
    "\n",
    "pd_pred_rf_sj = pd_pred_rf[pd_pred_rf['city'] == 'sj']\n",
    "pd_pred_rf_iq = pd_pred_rf[pd_pred_rf['city'] == 'iq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total cases per city\n",
    "plt.figure()\n",
    "\n",
    "plt.grid()\n",
    "plt.title('Random Forest: Prediction Dengue Fever total cases per week')\n",
    "\n",
    "\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('total cases')\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "\n",
    "plt.plot(pd_train_sj['week_start_date'], pd_train_sj['total_cases'], label='San Juan')\n",
    "plt.plot(pd_train_iq['week_start_date'], pd_train_iq['total_cases'], label='Iquitos')\n",
    "\n",
    "plt.plot(pd_pred_rf_sj['week_start_date'], pd_pred_rf_sj['total_cases'], label='Pred: San Juan')\n",
    "plt.plot(pd_pred_rf_iq['week_start_date'], pd_pred_rf_iq['total_cases'], label='Pred: Iquitos')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(stages=[indexer, encoder, vectorAssembler, scaler, lr])\n",
    "\n",
    "evaluator_lr = RegressionEvaluator(labelCol='total_cases',\n",
    "                                   predictionCol='lr_prediction',\n",
    "                                   metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = pipeline_lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prediction from 'test' dataset\n",
    "pred_lr = model_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr.select(['total_cases','lr_prediction']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator_lr.evaluate(pred_lr)\n",
    "\n",
    "print(\"Linear regression::Before grid search - RMSE = {:.3f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for linear regression (LR)\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "crossval_lr = CrossValidator(estimator=pipeline_lr,\n",
    "                             estimatorParamMaps=paramGrid_lr,\n",
    "                             evaluator=evaluator_lr,\n",
    "                             numFolds=2,\n",
    "                             seed=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model_lr = crossval_lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prediction from 'test' dataset\n",
    "cv_pred_lr = cv_model_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator_lr.evaluate(cv_pred_lr)\n",
    "\n",
    "print(\"Linear regression::After grid search - RMSE = {:.3f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a window\n",
    "w = Window().partitionBy().orderBy(F.col('week_start_date'))\n",
    "\n",
    "df_train2 = df_train\n",
    "#df_train2 = df_train.withColumn('station_avg_temp_1_c', F.lag('station_avg_temp_c', count=1).over(w)) \\\n",
    "#    .withColumn('station_avg_temp_2_c', F.lag('station_avg_temp_c', count=2).over(w)) \\\n",
    "#    .withColumn('station_avg_temp_3_c', F.lag('station_avg_temp_c', count=3).over(w)) \\\n",
    "#    .withColumn('station_avg_temp_4_c', F.lag('station_avg_temp_c', count=4).over(w))\n",
    "\n",
    "#df_train2 = df_train2.withColumn('station_precip_1_mm', F.lag('station_precip_mm', count=1).over(w)) \\\n",
    "#    .withColumn('station_precip_2_mm', F.lag('station_precip_mm', count=2).over(w)) \\\n",
    "#    .withColumn('station_precip_3_mm', F.lag('station_precip_mm', count=3).over(w)) \\\n",
    "#    .withColumn('station_precip_4_mm', F.lag('station_precip_mm', count=4).over(w))\n",
    "\n",
    "print(\"df_train2 = ({}, {})\".format(df_train2.count(), len(df_train2.columns)))\n",
    "\n",
    "#\n",
    "#df_train2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = df_train2.dropna()\n",
    "print(\"df_train2 = ({}, {})\".format(df_train2.count(), len(df_train2.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=['year', 'weekofyear',\n",
    "               'ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw',\n",
    "               'reanalysis_air_temp_k','reanalysis_avg_temp_k',\n",
    "               'reanalysis_dew_point_temp_k', 'reanalysis_max_air_temp_k',\n",
    "               'reanalysis_min_air_temp_k', 'reanalysis_precip_amt_kg_per_m2',\n",
    "               'reanalysis_relative_humidity_percent', 'reanalysis_sat_precip_amt_mm',\n",
    "               'reanalysis_specific_humidity_g_per_kg', 'reanalysis_tdtr_k',\n",
    "               'station_avg_temp_c',\n",
    "               'station_avg_temp_1_c', 'station_avg_temp_2_c', 'station_avg_temp_3_c', 'station_avg_temp_4_c',\n",
    "               'station_diur_temp_rng_c',\n",
    "               'station_max_temp_c', 'station_min_temp_c', \n",
    "               'station_precip_mm',\n",
    "               'station_precip_1_mm', 'station_precip_2_mm', 'station_precip_3_mm', 'station_precip_4_mm',\n",
    "               'cityVect'], \n",
    "    outputCol = 'features')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
